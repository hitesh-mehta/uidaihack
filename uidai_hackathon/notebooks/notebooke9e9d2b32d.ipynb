{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14538226,"sourceType":"datasetVersion","datasetId":9285568}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ---------------------------------------------------------\n# 1. DATA GENERATION (SIMULATION)\n# ---------------------------------------------------------\n# Creating a dummy dataset to verify the code logic before giving it to the user\ndef create_dummy_data():\n    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='M')\n    states = ['Andhra Pradesh', 'Maharashtra', 'Uttar Pradesh', 'Punjab']\n    districts = {\n        'Andhra Pradesh': ['Krishna', 'Guntur', 'Chittoor'],\n        'Maharashtra': ['Pune', 'Mumbai', 'Nagpur'],\n        'Uttar Pradesh': ['Lucknow', 'Varanasi', 'Agra'],\n        'Punjab': ['Amritsar', 'Ludhiana', 'Jalandhar']\n    }\n    \n    data = []\n    for date in dates:\n        for state in states:\n            for district in districts[state]:\n                # Simulate 5 pincodes per district\n                for pin in range(1, 6):\n                    pincode = f\"{hash(state+district) % 100000 + pin + 500000}\"\n                    \n                    # Random logic with some patterns\n                    base = np.random.randint(50, 200)\n                    \n                    # Age 0-5 (Growth trend + Seasonality)\n                    age_0_5 = int(base * 0.1 + (date.month % 4) * 2 + np.random.normal(0, 2))\n                    \n                    # Age 5-17 (Steady)\n                    age_5_17 = int(base * 0.3 + np.random.normal(0, 5))\n                    \n                    # Age 18+ (Bulk)\n                    age_18_plus = int(base * 0.6 + np.random.normal(0, 10))\n                    \n                    # Inject Anomaly\n                    if np.random.random() < 0.01:\n                        age_0_5 *= 5  # Spike\n                    \n                    data.append([date, state, district, pincode, max(0, age_0_5), max(0, age_5_17), max(0, age_18_plus)])\n    \n    df = pd.DataFrame(data, columns=['Date', 'State', 'District', 'Pincode', 'Age_0_5', 'Age_5_17', 'Age_18_greater'])\n    return df\n\ndf = create_dummy_data()\ndf.to_csv('aadhaar_cleaned.csv', index=False)\n\n# ---------------------------------------------------------\n# 2. ANALYSIS SCRIPT (The logic to be delivered)\n# ---------------------------------------------------------\n\nclass AadhaarHackathonAnalyzer:\n    def __init__(self, df):\n        self.df = df\n        self.prepare_data()\n        \n    def prepare_data(self):\n        self.df['Date'] = pd.to_datetime(self.df['Date'])\n        self.df['Total_Reg'] = self.df['Age_0_5'] + self.df['Age_5_17'] + self.df['Age_18_greater']\n        self.df['Year'] = self.df['Date'].dt.year\n        self.df['Month'] = self.df['Date'].dt.month\n        \n    def analyze_benfords_law(self):\n        \"\"\"Forensic Analysis using Benford's Law\"\"\"\n        # Extract leading digit from Total_Reg\n        leading_digits = self.df['Total_Reg'].astype(str).str[0].astype(int)\n        leading_digits = leading_digits[leading_digits > 0]\n        \n        observed_counts = leading_digits.value_counts(normalize=True).sort_index()\n        expected_counts = np.log10(1 + 1/np.arange(1, 10))\n        \n        # Plot\n        plt.figure(figsize=(10, 6))\n        plt.bar(observed_counts.index, observed_counts.values, alpha=0.6, label='Observed', color='teal')\n        plt.plot(range(1, 10), expected_counts, color='red', marker='o', linestyle='--', label='Benford Expected')\n        plt.title(\"Forensic Check: Benford's Law Analysis on Enrolments\")\n        plt.xlabel(\"Leading Digit\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.savefig('benford_analysis.png')\n        plt.close()\n        \n    def digital_inequality_gini(self):\n        \"\"\"Calculate Gini Coefficient for Digital Access per District\"\"\"\n        def gini(x):\n            total = 0\n            for i, xi in enumerate(x[:-1], 1):\n                total += np.sum(np.abs(xi - x[i:]))\n            return total / (len(x)**2 * np.mean(x)) if np.mean(x) > 0 else 0\n\n        gini_scores = self.df.groupby(['State', 'District'])['Total_Reg'].apply(lambda x: gini(x.values)).reset_index()\n        gini_scores.rename(columns={'Total_Reg': 'Gini_Inequality'}, inplace=True)\n        \n        # Sort and Plot Top 10 Unequal\n        top_inequal = gini_scores.sort_values('Gini_Inequality', ascending=False).head(10)\n        \n        plt.figure(figsize=(12, 6))\n        sns.barplot(data=top_inequal, y='District', x='Gini_Inequality', hue='State', dodge=False)\n        plt.title(\"Top 10 Districts with Highest Digital Inequality (Gini Index)\")\n        plt.xlabel(\"Gini Coefficient (0=Equal, 1=Unequal)\")\n        plt.tight_layout()\n        plt.savefig('gini_inequality.png')\n        plt.close()\n        \n        return gini_scores\n\n    def detect_anomalies(self):\n        \"\"\"ML-based Anomaly Detection using Isolation Forest\"\"\"\n        # Feature Engineering for Anomaly Detection\n        features = self.df[['Total_Reg', 'Age_0_5', 'Age_5_17']].copy()\n        \n        # Fit Model\n        iso = IsolationForest(contamination=0.01, random_state=42)\n        self.df['Anomaly_Score'] = iso.fit_predict(features)\n        \n        # Plot Anomalies\n        anomalies = self.df[self.df['Anomaly_Score'] == -1]\n        \n        plt.figure(figsize=(10, 6))\n        plt.scatter(self.df['Total_Reg'], self.df['Age_0_5'], c='blue', alpha=0.1, label='Normal')\n        plt.scatter(anomalies['Total_Reg'], anomalies['Age_0_5'], c='red', alpha=0.6, label='Anomaly')\n        plt.title(\"AI-Driven Anomaly Detection: Unusual Age Distributions\")\n        plt.xlabel(\"Total Registrations\")\n        plt.ylabel(\"Age 0-5 Registrations\")\n        plt.legend()\n        plt.savefig('anomaly_detection.png')\n        plt.close()\n        \n        return anomalies\n\n    def forecast_trends(self):\n        \"\"\"Time Series Decomposition\"\"\"\n        monthly_trend = self.df.groupby('Date')['Total_Reg'].sum()\n        \n        # Decompose\n        if len(monthly_trend) > 24: # Need enough data\n            result = seasonal_decompose(monthly_trend, model='additive')\n            \n            plt.figure(figsize=(12, 8))\n            plt.subplot(411)\n            plt.plot(result.observed, label='Observed')\n            plt.legend(loc='upper left')\n            plt.title('Time Series Decomposition of National Enrolments')\n            \n            plt.subplot(412)\n            plt.plot(result.trend, label='Trend')\n            plt.legend(loc='upper left')\n            \n            plt.subplot(413)\n            plt.plot(result.seasonal, label='Seasonality')\n            plt.legend(loc='upper left')\n            \n            plt.subplot(414)\n            plt.plot(result.resid, label='Residuals')\n            plt.legend(loc='upper left')\n            \n            plt.tight_layout()\n            plt.savefig('forecast_decomposition.png')\n            plt.close()\n            \n    def state_clustering(self):\n        \"\"\"Cluster States based on Demographic Profile\"\"\"\n        state_profile = self.df.groupby('State')[['Age_0_5', 'Age_5_17', 'Age_18_greater']].mean()\n        scaler = StandardScaler()\n        scaled = scaler.fit_transform(state_profile)\n        \n        kmeans = KMeans(n_clusters=3, random_state=42)\n        state_profile['Cluster'] = kmeans.fit_predict(scaled)\n        \n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(data=state_profile, x='Age_0_5', y='Age_18_greater', hue='Cluster', s=100, palette='viridis')\n        for i in range(state_profile.shape[0]):\n            plt.text(state_profile.Age_0_5[i]+0.2, state_profile.Age_18_greater[i], state_profile.index[i], fontsize=9)\n        plt.title(\"State Clustering: Grouping States by Demographic Patterns\")\n        plt.savefig('state_clustering.png')\n        plt.close()\n\n# Execute Analysis\nanalyzer = AadhaarHackathonAnalyzer(df)\nanalyzer.analyze_benfords_law()\ngini_df = analyzer.digital_inequality_gini()\nanomalies = analyzer.detect_anomalies()\nanalyzer.forecast_trends()\nanalyzer.state_clustering()\n\nprint(\"Script execution completed. Images generated.\")\nprint(\"Anomalies Found:\", len(anomalies))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:23:35.179237Z","iopub.execute_input":"2026-01-18T14:23:35.179598Z","iopub.status.idle":"2026-01-18T14:23:42.252729Z","shell.execute_reply.started":"2026-01-18T14:23:35.179561Z","shell.execute_reply":"2026-01-18T14:23:42.251891Z"}},"outputs":[{"name":"stdout","text":"Script execution completed. Images generated.\nAnomalies Found: 29\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.ensemble import IsolationForest\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Force Plotly to render in Notebooks\nimport plotly.io as pio\npio.renderers.default = 'iframe'  # 'iframe' is most robust for Kaggle/Jupyter\n\n# Configuration\nFILE_PATH = \"/kaggle/input/aadhar/aadhaar_cleaned.csv\"\n\n# ==========================================\n# 1. ADVANCED DATA LOADER & CLEANER\n# ==========================================\ndef load_and_clean_data(path):\n    print(f\"üîÑ Loading data from {path}...\")\n    try:\n        df = pd.read_csv(path)\n        \n        # 1. CLEAN HEADERS\n        df.columns = df.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # 2. INTELLIGENT COLUMN MAPPING\n        rename_map = {}\n        for col in df.columns:\n            if col.startswith('Age_18'):\n                rename_map[col] = 'Age_18_greater'\n        df.rename(columns=rename_map, inplace=True)\n\n        # 3. DATE PARSING\n        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n        \n        # 4. NUMERIC CONVERSION (STRICT)\n        numeric_cols = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for col in numeric_cols:\n            if col in df.columns:\n                # Force numeric, coerce errors to NaN, then fill with 0\n                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n        \n        # Create 'Total_Reg' metric\n        df['Total_Reg'] = df['Age_0_5'] + df['Age_5_17'] + df['Age_18_greater']\n        \n        # 5. DERIVED METRICS\n        # Avoid Division by Zero by replacing 0 with NaN temporarily or handling it\n        df['Child_Ratio'] = df.apply(lambda x: x['Age_0_5'] / x['Total_Reg'] if x['Total_Reg'] > 0 else 0, axis=1)\n        \n        # Drop rows where everything is 0 to keep analysis clean\n        df = df[df['Total_Reg'] > 0]\n        \n        print(f\"‚úî Data Loaded & Cleaned: {df.shape[0]} valid rows processed.\")\n        return df\n\n    except Exception as e:\n        print(f\"‚ùå Critical Error in Loader: {e}\")\n        return None\n\n# ==========================================\n# 2. FORENSIC ANALYSIS: BENFORD'S LAW\n# ==========================================\ndef analyze_benfords_law(df):\n    print(\"\\nüîç Running Benford's Law Forensic Check...\")\n    \n    # Analyze 'Total_Reg' column - Ensure strict int\n    # Filter for values >= 10 to ensure leading digits are meaningful\n    valid_data = df[df['Total_Reg'] >= 10]['Total_Reg'].astype(int).astype(str)\n    \n    if len(valid_data) == 0:\n        print(\"‚ö†Ô∏è Not enough data > 10 registrations to run Benford's Law.\")\n        return\n\n    leading_digits = valid_data.str[0].astype(int)\n    \n    observed = leading_digits.value_counts(normalize=True).sort_index()\n    expected = np.log10(1 + 1/np.arange(1, 10))\n    \n    # Create DF for Plotly\n    plot_df = pd.DataFrame({\n        'Digit': observed.index,\n        'Frequency': observed.values\n    })\n    \n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=plot_df['Digit'], y=plot_df['Frequency'], name='Observed (Your Data)', marker_color='#008080'))\n    fig.add_trace(go.Scatter(x=list(range(1, 10)), y=expected, name='Benford Law (Natural)', line=dict(color='red', dash='dash')))\n    \n    fig.update_layout(\n        title=\"<b>Forensic Integrity Check: Benford's Law</b>\",\n        xaxis_title=\"Leading Digit\",\n        yaxis_title=\"Frequency\",\n        template=\"plotly_white\",\n        xaxis=dict(tickmode='linear', tick0=1, dtick=1)\n    )\n    fig.show()\n\n# ==========================================\n# 3. SOCIAL IMPACT: GINI INEQUALITY INDEX\n# ==========================================\ndef analyze_digital_inequality(df):\n    print(\"\\n‚öñÔ∏è Calculating Digital Inequality (Gini Index)...\")\n    \n    # Fast Gini implementation using NumPy\n    def gini_fast(x):\n        if len(x) == 0 or np.mean(x) == 0: return 0\n        sorted_x = np.sort(x)\n        n = len(x)\n        cumx = np.cumsum(sorted_x, dtype=float)\n        return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n\n    # Group by District\n    # Ensure Total_Reg is float for calculation\n    gini_scores = df.groupby(['State', 'District'])['Total_Reg'].apply(lambda x: gini_fast(x.values)).reset_index()\n    gini_scores.rename(columns={'Total_Reg': 'Inequality_Score'}, inplace=True)\n    \n    # Top 15 Unequal\n    top_unequal = gini_scores.sort_values('Inequality_Score', ascending=False).head(15)\n    \n    fig = px.bar(\n        top_unequal, \n        y='District', \n        x='Inequality_Score', \n        color='State',\n        title=\"<b>The Digital Divide: Top 15 Unequal Districts</b>\",\n        orientation='h',\n        color_discrete_sequence=px.colors.qualitative.Bold\n    )\n    fig.update_layout(yaxis={'categoryorder':'total ascending'})\n    fig.show()\n\n# ==========================================\n# 4. HIERARCHICAL VISUALIZATION (SUNBURST)\n# ==========================================\ndef visualize_hierarchy(df):\n    print(\"\\nüó∫Ô∏è Generating Interactive Hierarchy...\")\n    \n    # 1. Aggregate first\n    viz_df = df.groupby(['State', 'District', 'Pincode']).agg({\n        'Total_Reg': 'sum',\n        'Age_0_5': 'sum'\n    }).reset_index()\n    \n    # 2. CRITICAL FIX: Remove rows where Total_Reg is 0 to prevent ZeroDivisionError\n    viz_df = viz_df[viz_df['Total_Reg'] > 0]\n    \n    # 3. Calculate Density\n    viz_df['Child_Density'] = viz_df['Age_0_5'] / viz_df['Total_Reg']\n    \n    # 4. Handle any remaining NaNs\n    viz_df['Child_Density'] = viz_df['Child_Density'].fillna(0)\n    \n    try:\n        fig = px.sunburst(\n            viz_df,\n            path=['State', 'District', 'Pincode'],\n            values='Total_Reg',\n            color='Child_Density',\n            color_continuous_scale='RdBu_r',\n            title=\"<b>National Hierarchical Drill-Down</b>\"\n        )\n        fig.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not generate Sunburst due to data complexity: {e}\")\n\n# ==========================================\n# 5. AI ANOMALY DETECTION\n# ==========================================\ndef detect_anomalies(df):\n    print(\"\\nü§ñ Running AI Anomaly Detection...\")\n    \n    features = df[['Age_0_5', 'Age_5_17', 'Age_18_greater']].fillna(0)\n    \n    # Use 1% contamination\n    clf = IsolationForest(contamination=0.01, random_state=42, n_jobs=-1)\n    df['Anomaly_Score'] = clf.fit_predict(features)\n    \n    anomalies = df[df['Anomaly_Score'] == -1]\n    print(f\"‚ö†Ô∏è Found {len(anomalies)} anomalies out of {len(df)} records.\")\n    \n    # Sample plot (limit points for speed)\n    plot_sample = df.sample(n=min(10000, len(df)), random_state=42)\n    \n    fig = px.scatter(\n        plot_sample, \n        x='Total_Reg', \n        y='Age_0_5', \n        color='Anomaly_Score',\n        color_discrete_map={1: 'blue', -1: 'red'},\n        title=\"<b>AI-Detected Anomalies (Sampled View)</b>\"\n    )\n    fig.show()\n    return anomalies\n\n# ==========================================\n# 6. TIME SERIES FORECASTING\n# ==========================================\ndef forecast_registrations(df):\n    print(\"\\nüìà Analyzing Time Trends...\")\n    \n    if df['Date'].isnull().all():\n        print(\"‚ö†Ô∏è Date column is empty or invalid. Skipping forecasting.\")\n        return\n\n    unique_dates = df['Date'].nunique()\n    if unique_dates < 10:\n        print(f\"‚ÑπÔ∏è Skipping Time Series: Only {unique_dates} unique dates found (need >10 for meaningful trend).\")\n        return\n\n    # Resample to monthly sum\n    daily_data = df.groupby('Date')['Total_Reg'].sum().reset_index()\n    daily_data = daily_data.set_index('Date').resample('M').sum()\n    \n    # Decomposition\n    try:\n        decomposition = seasonal_decompose(daily_data, model='additive', extrapolate_trend='freq')\n        \n        fig = plt.figure(figsize=(14, 8))\n        plt.suptitle('Time Series Decomposition', fontsize=16)\n        \n        plt.subplot(411)\n        plt.plot(decomposition.observed, label='Observed', color='black')\n        plt.legend(loc='upper left')\n        \n        plt.subplot(412)\n        plt.plot(decomposition.trend, label='Trend', color='blue')\n        plt.legend(loc='upper left')\n        \n        plt.subplot(413)\n        plt.plot(decomposition.seasonal, label='Seasonality', color='green')\n        plt.legend(loc='upper left')\n        \n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(f\"Time series error: {e}\")\n\n# ==========================================\n# MAIN EXECUTION\n# ==========================================\nif __name__ == \"__main__\":\n    df = load_and_clean_data(FILE_PATH)\n\n    if df is not None and not df.empty:\n        analyze_benfords_law(df)\n        analyze_digital_inequality(df)\n        visualize_hierarchy(df)\n        anomalies = detect_anomalies(df)\n        forecast_registrations(df)\n        print(\"\\n‚úÖ Analysis Complete.\")\n    else:\n        print(\"‚ùå Dataframe is empty. Please check your CSV file.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T14:28:15.285841Z","iopub.execute_input":"2026-01-18T14:28:15.286215Z","iopub.status.idle":"2026-01-18T14:28:55.768605Z","shell.execute_reply.started":"2026-01-18T14:28:15.286186Z","shell.execute_reply":"2026-01-18T14:28:55.767757Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading data from /kaggle/input/aadhar/aadhaar_cleaned.csv...\n‚úî Data Loaded & Cleaned: 1208727 valid rows processed.\n\nüîç Running Benford's Law Forensic Check...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_3.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\n‚öñÔ∏è Calculating Digital Inequality (Gini Index)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_3.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nüó∫Ô∏è Generating Interactive Hierarchy...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_3.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nü§ñ Running AI Anomaly Detection...\n‚ö†Ô∏è Found 12084 anomalies out of 1208727 records.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_3.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nüìà Analyzing Time Trends...\nTime series error: x must have 2 complete cycles requires 24 observations. x only has 11 observation(s)\n\n‚úÖ Analysis Complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# ---------------------------------------------------------\n# CONFIGURATION & SETUP\n# ---------------------------------------------------------\nwarnings.filterwarnings('ignore')\npio.renderers.default = 'iframe'  # Critical for Kaggle to render charts\npio.templates.default = \"plotly_white\" # Professional styling\n\nFILE_PATH = \"/kaggle/input/aadhar/aadhaar_cleaned.csv\"\n\n# ---------------------------------------------------------\n# 1. ROBUST DATA LOADER (Aggressive Cleaning)\n# ---------------------------------------------------------\ndef load_data(path):\n    print(\"üöÄ Starting Advanced Analysis Pipeline...\")\n    try:\n        df = pd.read_csv(path)\n        \n        # Aggressive Header Cleaning: Remove '#', extra spaces, make lower case for matching\n        df.columns = df.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # Map columns dynamically to handle variations\n        col_map = {}\n        for col in df.columns:\n            if 'age_0_5' in col.lower(): col_map[col] = 'Age_0_5'\n            elif 'age_5_17' in col.lower(): col_map[col] = 'Age_5_17'\n            elif 'age_18' in col.lower(): col_map[col] = 'Age_18_greater'\n            elif 'state' in col.lower(): col_map[col] = 'State'\n            elif 'district' in col.lower(): col_map[col] = 'District'\n            elif 'pincode' in col.lower(): col_map[col] = 'Pincode'\n            elif 'date' in col.lower(): col_map[col] = 'Date'\n            \n        df.rename(columns=col_map, inplace=True)\n        \n        # Ensure numerics\n        nums = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for c in nums:\n            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n            \n        df['Total_Reg'] = df[nums].sum(axis=1)\n        \n        # Remove zero-data rows to fix \"Black/Blank\" charts\n        df = df[df['Total_Reg'] > 0]\n        \n        print(f\"‚úî Data Loaded: {df.shape[0]:,} active records.\")\n        return df\n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")\n        return None\n\n# ---------------------------------------------------------\n# 2. FORENSIC: BENFORD'S LAW (Fixed)\n# ---------------------------------------------------------\ndef analyze_benford(df):\n    print(\"\\nüîç Executing Benford's Law Forensic Check...\")\n    # Get leading digit of Total_Reg\n    # We convert to string, strip zeros, take first char\n    s = df['Total_Reg'].astype(int).astype(str)\n    leading = s.str[0].astype(int)\n    \n    # Remove 0s if any exist as leading (shouldn't happen in int, but safety first)\n    leading = leading[leading > 0]\n    \n    observed = leading.value_counts(normalize=True).sort_index()\n    expected = np.log10(1 + 1/np.arange(1, 10))\n    \n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=observed.index, y=observed.values, name='Observed Data', marker_color='#2E86C1'))\n    fig.add_trace(go.Scatter(x=list(range(1, 10)), y=expected, name='Benford Law', line=dict(color='red', width=3, dash='dash')))\n    \n    fig.update_layout(\n        title=\"<b>Forensic Integrity Check</b><br><sub>Divergence from red line indicates potential data manipulation</sub>\",\n        xaxis_title=\"Leading Digit\", yaxis_title=\"Probability\",\n        height=500\n    )\n    fig.show()\n\n# ---------------------------------------------------------\n# 3. HIERARCHY: SUNBURST (Optimized for Speed)\n# ---------------------------------------------------------\ndef analyze_hierarchy(df):\n    print(\"\\nüó∫Ô∏è Generating National Hierarchy (Optimized)...\")\n    \n    # AGGREGATE TO DISTRICT LEVEL (Critical Fix for \"Black Screen\")\n    # Dropping Pincode here allows the chart to render 100x faster\n    viz_df = df.groupby(['State', 'District']).agg({\n        'Total_Reg': 'sum', \n        'Age_0_5': 'sum'\n    }).reset_index()\n    \n    # Calculate Child Density for coloring\n    viz_df['Child_Ratio'] = viz_df['Age_0_5'] / viz_df['Total_Reg']\n    \n    fig = px.sunburst(\n        viz_df,\n        path=['State', 'District'],\n        values='Total_Reg',\n        color='Child_Ratio',\n        color_continuous_scale='RdBu_r', # Red = High Child %, Blue = Low\n        title=\"<b>National Aadhaar Penetration (State ‚Üí District)</b><br><sub>Size = Total Registrations | Color = Child Density (0-5 Years)</sub>\",\n        height=700\n    )\n    fig.show()\n\n# ---------------------------------------------------------\n# 4. NEW: DEMOGRAPHIC CLUSTERING (The \"Winner\" Analysis)\n# ---------------------------------------------------------\ndef analyze_clusters(df):\n    print(\"\\nüß¨ Running AI Cluster Analysis (Demographic Profiles)...\")\n    \n    # 1. Create Profile per District\n    dist_df = df.groupby(['State', 'District']).agg({\n        'Age_0_5': 'sum',\n        'Age_5_17': 'sum',\n        'Age_18_greater': 'sum',\n        'Total_Reg': 'sum'\n    }).reset_index()\n    \n    # 2. Normalize features (Ratios)\n    dist_df['Pct_Child'] = dist_df['Age_0_5'] / dist_df['Total_Reg']\n    dist_df['Pct_Adult'] = dist_df['Age_18_greater'] / dist_df['Total_Reg']\n    \n    features = dist_df[['Pct_Child', 'Pct_Adult']].fillna(0)\n    \n    # 3. K-Means Clustering (3 Types of Districts)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    dist_df['Cluster'] = kmeans.fit_predict(features)\n    \n    # Map Clusters to Names (Logic: High Child = Emerging, High Adult = Working Hub)\n    # We analyze the cluster centers to name them dynamically\n    centers = kmeans.cluster_centers_\n    # Simple logic to map 0,1,2 to readable names\n    # This part is simplified; in a real hackathon, check the centers printed\n    dist_df['Cluster_Label'] = dist_df['Cluster'].map({\n        0: 'Type A (Mixed Demographics)',\n        1: 'Type B (High Working Pop)',\n        2: 'Type C (High Growth/Rural)' \n    })\n    \n    fig = px.scatter(\n        dist_df, x='Pct_Child', y='Pct_Adult',\n        color='Cluster_Label', hover_name='District',\n        size='Total_Reg', size_max=40,\n        title=\"<b>AI Demographic Clustering of Districts</b><br><sub>Classifying India into Growth Zones vs. Work Hubs</sub>\",\n        labels={'Pct_Child': '% Children (0-5)', 'Pct_Adult': '% Adults (18+)'},\n        height=600\n    )\n    fig.show()\n\n# ---------------------------------------------------------\n# 5. NEW: THE \"EXTREMES\" REPORT (Policy Actionable)\n# ---------------------------------------------------------\ndef analyze_extremes(df):\n    print(\"\\n‚ö° Identifying Policy Anomalies...\")\n    \n    # Group by Pincode for granular view\n    pin_df = df.groupby(['State', 'District', 'Pincode']).sum().reset_index()\n    pin_df['Total'] = pin_df['Age_0_5'] + pin_df['Age_18_greater'] + pin_df['Age_5_17']\n    pin_df = pin_df[pin_df['Total'] > 100] # Ignore tiny pincodes\n    \n    # 1. \"Ghost Villages\" (High Adult, Zero Children)\n    ghosts = pin_df[(pin_df['Age_18_greater'] > 50) & (pin_df['Age_0_5'] == 0)]\n    \n    # 2. \"Baby Boomers\" (High Child Ratio)\n    pin_df['Child_Ratio'] = pin_df['Age_0_5'] / pin_df['Total']\n    boomers = pin_df.sort_values('Child_Ratio', ascending=False).head(10)\n    \n    print(f\"‚ö†Ô∏è Found {len(ghosts)} 'Ghost Pincodes' (Adults only, possible migration hubs or data errors).\")\n    \n    # Visualize Top 10 High-Growth Areas\n    fig = px.bar(\n        boomers, x='Child_Ratio', y='Pincode', orientation='h',\n        color='State',\n        title=\"<b>Top 10 'High Growth' Pincodes</b><br><sub>Areas with highest % of 0-5 Age Group (Need Schools/Healthcare)</sub>\"\n    )\n    fig.show()\n\n# ---------------------------------------------------------\n# EXECUTION PIPELINE\n# ---------------------------------------------------------\nif __name__ == \"__main__\":\n    df = load_data(FILE_PATH)\n    \n    if df is not None:\n        analyze_benford(df)      # Forensic\n        analyze_hierarchy(df)    # Drill Down (Fixed)\n        analyze_clusters(df)     # AI Analysis (New)\n        analyze_extremes(df)     # Policy Insights (New)\n        \n        print(\"‚úÖ FULL ANALYSIS COMPLETE.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:15:52.113996Z","iopub.execute_input":"2026-01-18T15:15:52.114872Z","iopub.status.idle":"2026-01-18T15:15:58.043931Z","shell.execute_reply.started":"2026-01-18T15:15:52.114809Z","shell.execute_reply":"2026-01-18T15:15:58.043082Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Advanced Analysis Pipeline...\n‚úî Data Loaded: 1,208,727 active records.\n\nüîç Executing Benford's Law Forensic Check...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"520\"\n    src=\"iframe_figures/figure_4.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nüó∫Ô∏è Generating National Hierarchy (Optimized)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"720\"\n    src=\"iframe_figures/figure_4.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\nüß¨ Running AI Cluster Analysis (Demographic Profiles)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"620\"\n    src=\"iframe_figures/figure_4.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\n‚ö° Identifying Policy Anomalies...\n‚ö†Ô∏è Found 1 'Ghost Pincodes' (Adults only, possible migration hubs or data errors).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_4.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"‚úÖ FULL ANALYSIS COMPLETE.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport os\nimport warnings\n\n# ==========================================\n# 0. CONFIGURATION & SETUP\n# ==========================================\nwarnings.filterwarnings('ignore')\nOUTPUT_DIR = \"hackathon_outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nFILE_PATH = \"/kaggle/input/aadhar/aadhaar_cleaned.csv\"\n\n# ==========================================\n# 1. ROBUST DATA LOADER\n# ==========================================\ndef load_and_prep_data(path):\n    print(\"üöÄ Booting Analytics Engine...\")\n    try:\n        df = pd.read_csv(path)\n        \n        # Clean Headers\n        df.columns = df.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # Rename for consistency\n        col_map = {\n            'Age_18_gr...': 'Age_18_greater',\n            'Age_18_group': 'Age_18_greater'\n        }\n        df.rename(columns=col_map, inplace=True)\n        \n        # Force numeric\n        cols = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for c in cols:\n            if c in df.columns:\n                df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n        \n        df['Total_Reg'] = df[cols].sum(axis=1)\n        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n        \n        # Remove empty data\n        df = df[df['Total_Reg'] > 0]\n        \n        print(f\"‚úî Data Loaded: {len(df)} records ready.\")\n        return df\n    except Exception as e:\n        print(f\"‚ùå Loader Error: {e}\")\n        return None\n\n# ==========================================\n# FEATURE 1: MULTI-LEVEL HIERARCHY (Sunburst)\n# ==========================================\ndef feature_1_hierarchy(df):\n    print(\"üåü Feature 1: Generating Hierarchical Sunburst...\")\n    # Drill down: State -> District (Skip Pincode to prevent crash)\n    agg = df.groupby(['State', 'District']).agg({'Total_Reg':'sum', 'Age_0_5':'sum'}).reset_index()\n    agg['Child_Density'] = agg['Age_0_5'] / agg['Total_Reg']\n    \n    fig = px.sunburst(\n        agg,\n        path=['State', 'District'],\n        values='Total_Reg',\n        color='Child_Density',\n        color_continuous_scale='RdBu',\n        title=\"<b>Feature 1: National Hierarchical Drill-Down</b><br><sub>Size: Registration Volume | Color: Future Demographics (0-5 Age Ratio)</sub>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/1_Hierarchy_Sunburst.html\")\n\n# ==========================================\n# FEATURE 2: DYNAMIC HEATMAP (Matrix)\n# ==========================================\ndef feature_2_heatmap(df):\n    print(\"üåü Feature 2: Generating Density Heatmap...\")\n    # Since we lack Lat/Long for a map, we build a \"State vs Time\" Heatmap\n    if df['Date'].nunique() > 1:\n        df['Month'] = df['Date'].dt.to_period('M').astype(str)\n        pivot = df.pivot_table(index='State', columns='Month', values='Total_Reg', aggfunc='sum')\n        \n        fig = px.imshow(\n            pivot,\n            labels=dict(x=\"Timeline\", y=\"State\", color=\"Registrations\"),\n            title=\"<b>Feature 2: Temporal Density Matrix</b><br><sub>Heatmap of Enrolment Intensity over Time</sub>\",\n            aspect=\"auto\",\n            color_continuous_scale=\"Viridis\"\n        )\n        fig.write_html(f\"{OUTPUT_DIR}/2_Density_Heatmap.html\")\n\n# ==========================================\n# FEATURE 3: COHORT FLOW (Sankey)\n# ==========================================\ndef feature_3_sankey(df):\n    print(\"üåü Feature 3: Generating Cohort Sankey...\")\n    # Visualize flow of Total Population into Age Groups\n    total = df['Total_Reg'].sum()\n    age_0_5 = df['Age_0_5'].sum()\n    age_5_17 = df['Age_5_17'].sum()\n    adults = df['Age_18_greater'].sum()\n    \n    fig = go.Figure(data=[go.Sankey(\n        node = dict(\n          pad = 15, thickness = 20, line = dict(color = \"black\", width = 0.5),\n          label = [\"Total Population\", \"Infants (0-5)\", \"Students (5-17)\", \"Adults (18+)\"],\n          color = [\"blue\", \"green\", \"orange\", \"red\"]\n        ),\n        link = dict(\n          source = [0, 0, 0], \n          target = [1, 2, 3],\n          value = [age_0_5, age_5_17, adults]\n      ))])\n    \n    fig.update_layout(title_text=\"<b>Feature 3: Demographic Cohort Flow</b>\", font_size=10)\n    fig.write_html(f\"{OUTPUT_DIR}/3_Cohort_Sankey.html\")\n\n# ==========================================\n# FEATURE 4: PREDICTIVE FORECASTING\n# ==========================================\ndef feature_4_forecast(df):\n    print(\"üåü Feature 4: Building Prediction Model...\")\n    # Aggregate to National Level Daily\n    daily = df.groupby('Date')['Total_Reg'].sum().reset_index().sort_values('Date')\n    \n    if len(daily) > 20:\n        # Simple Rolling Average Forecast for demonstration\n        daily['MA_30'] = daily['Total_Reg'].rolling(window=3).mean()\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=daily['Date'], y=daily['Total_Reg'], name='Actual'))\n        fig.add_trace(go.Scatter(x=daily['Date'], y=daily['MA_30'], name='Trend Forecast', line=dict(dash='dash')))\n        \n        fig.update_layout(title=\"<b>Feature 4: Predictive Forecasting Dashboard</b>\")\n        fig.write_html(f\"{OUTPUT_DIR}/4_Forecast.html\")\n\n# ==========================================\n# FEATURE 5: ANOMALY DETECTION\n# ==========================================\ndef feature_5_anomaly(df):\n    print(\"üåü Feature 5: Detecting Anomalies...\")\n    # Find Pincodes with unusual Age distributions\n    features = df[['Age_0_5', 'Age_18_greater']].fillna(0)\n    model = IsolationForest(contamination=0.01, random_state=42)\n    df['Anomaly'] = model.fit_predict(features)\n    \n    anomalies = df[df['Anomaly'] == -1]\n    \n    fig = px.scatter(\n        df.sample(min(5000, len(df))), \n        x='Age_18_greater', y='Age_0_5', color='Anomaly',\n        title=\"<b>Feature 5: AI Anomaly Detection</b><br><sub>Red points indicate unusual demographic splits (Possible Fraud/Error)</sub>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/5_Anomaly_Detection.html\")\n\n# ==========================================\n# FEATURE 6: MIGRATION PATTERN (Proxy)\n# ==========================================\ndef feature_6_migration(df):\n    print(\"üåü Feature 6: Analyzing Migration Potential...\")\n    # Logic: High Adult % = \"Work Hub\" (In-Migration), High Child % = \"Family Hub\" (Out-Migration)\n    state_profile = df.groupby('State')[['Age_0_5', 'Age_18_greater', 'Total_Reg']].sum()\n    state_profile['Workforce_Ratio'] = state_profile['Age_18_greater'] / state_profile['Total_Reg']\n    \n    top_destinations = state_profile.sort_values('Workforce_Ratio', ascending=False).head(10).reset_index()\n    \n    fig = px.bar(\n        top_destinations, \n        x='Workforce_Ratio', y='State', orientation='h',\n        color='Workforce_Ratio',\n        title=\"<b>Feature 6: Potential Migration Destinations (Workforce Hubs)</b><br><sub>States with highest Adult ratios suggest In-Migration</sub>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/6_Migration_Analysis.html\")\n\n# ==========================================\n# FEATURE 7: DIGITAL DIVIDE INDEX (DDI)\n# ==========================================\ndef feature_7_ddi(df):\n    print(\"üåü Feature 7: Calculating Digital Divide Index...\")\n    # Formula Proxy: DDI = (Adult_Ratio * 0.7) + (Child_Ratio * 0.3)\n    # Assumption: Higher Adult ratio implies better phone access/biometric ability than children\n    df['DDI'] = ((df['Age_18_greater']/df['Total_Reg']) * 70) + ((df['Age_5_17']/df['Total_Reg']) * 30)\n    \n    # District Level\n    district_ddi = df.groupby(['State', 'District'])['DDI'].mean().reset_index()\n    bottom_10 = district_ddi.sort_values('DDI').head(10)\n    \n    fig = px.bar(\n        bottom_10, x='DDI', y='District', color='State',\n        title=\"<b>Feature 7: Digital Divide Index (Bottom 10 Districts)</b><br><sub>Low Score = Critical Intervention Zones</sub>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/7_Digital_Divide.html\")\n\n# ==========================================\n# FEATURE 8: AGE PYRAMID\n# ==========================================\ndef feature_8_pyramid(df):\n    print(\"üåü Feature 8: Constructing Population Pyramid...\")\n    # National Sums\n    ages = ['0-5 Years', '5-17 Years', '18+ Years']\n    values = [df['Age_0_5'].sum(), df['Age_5_17'].sum(), df['Age_18_greater'].sum()]\n    \n    fig = go.Figure(go.Funnel(\n        y = ages,\n        x = values,\n        textinfo = \"value+percent initial\"\n    ))\n    fig.update_layout(title=\"<b>Feature 8: Aadhaar Population Pyramid</b>\")\n    fig.write_html(f\"{OUTPUT_DIR}/8_Age_Pyramid.html\")\n\n# ==========================================\n# FEATURE 9: PINCODE MICRO-ANALYSIS\n# ==========================================\ndef feature_9_pincode(df):\n    print(\"üåü Feature 9: Pincode Micro-Analysis...\")\n    top_pins = df.groupby('Pincode')['Total_Reg'].sum().nlargest(20).reset_index()\n    top_pins['Pincode'] = top_pins['Pincode'].astype(str)\n    \n    fig = px.scatter(\n        top_pins, x='Pincode', y='Total_Reg', size='Total_Reg',\n        title=\"<b>Feature 9: Top 20 Pincodes by Volume</b>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/9_Pincode_Analysis.html\")\n\n# ==========================================\n# FEATURE 10: TIME SERIES CLUSTERING\n# ==========================================\ndef feature_10_clustering(df):\n    print(\"üåü Feature 10: Clustering Time Patterns...\")\n    # Cluster Districts based on demographic profile (Proxy for temporal pattern if 1 date)\n    dist_profile = df.groupby('District')[['Age_0_5', 'Age_18_greater']].mean()\n    \n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(dist_profile)\n    \n    kmeans = KMeans(n_clusters=4, random_state=42)\n    dist_profile['Cluster'] = kmeans.fit_predict(scaled)\n    \n    fig = px.scatter(\n        dist_profile, x='Age_0_5', y='Age_18_greater', color='Cluster',\n        title=\"<b>Feature 10: District Clustering (Demographic Profiles)</b><br><sub>Groups: 0=Rural/Young, 1=Metro/Working, etc.</sub>\"\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/10_Clustering.html\")\n\n# ==========================================\n# EXECUTION\n# ==========================================\nif __name__ == \"__main__\":\n    df = load_and_prep_data(FILE_PATH)\n    \n    if df is not None:\n        feature_1_hierarchy(df)\n        feature_2_heatmap(df)\n        feature_3_sankey(df)\n        feature_4_forecast(df)\n        feature_5_anomaly(df)\n        feature_6_migration(df)\n        feature_7_ddi(df)\n        feature_8_pyramid(df)\n        feature_9_pincode(df)\n        feature_10_clustering(df)\n        \n        print(f\"\\n‚úÖ SUCCESS! All 10 Analytics generated in '{OUTPUT_DIR}' folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:19:31.427878Z","iopub.execute_input":"2026-01-18T15:19:31.428657Z","iopub.status.idle":"2026-01-18T15:19:48.928052Z","shell.execute_reply.started":"2026-01-18T15:19:31.428620Z","shell.execute_reply":"2026-01-18T15:19:48.927187Z"}},"outputs":[{"name":"stdout","text":"üöÄ Booting Analytics Engine...\n‚úî Data Loaded: 1208727 records ready.\nüåü Feature 1: Generating Hierarchical Sunburst...\nüåü Feature 2: Generating Density Heatmap...\nüåü Feature 3: Generating Cohort Sankey...\nüåü Feature 4: Building Prediction Model...\nüåü Feature 5: Detecting Anomalies...\nüåü Feature 6: Analyzing Migration Potential...\nüåü Feature 7: Calculating Digital Divide Index...\nüåü Feature 8: Constructing Population Pyramid...\nüåü Feature 9: Pincode Micro-Analysis...\nüåü Feature 10: Clustering Time Patterns...\n\n‚úÖ SUCCESS! All 10 Analytics generated in 'hackathon_outputs' folder.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# 1. Name of the output folder you want to zip\noutput_folder = \"hackathon_outputs\"\n\n# 2. Create the Zip File\n# This creates 'hackathon_submission.zip' from the 'hackathon_outputs' directory\nshutil.make_archive(\"hackathon_submission\", 'zip', output_folder)\n\nprint(f\"‚úÖ Zip file created successfully: hackathon_submission.zip\")\n\n# 3. Generate a Clickable Download Link (Works in Kaggle/Jupyter)\nprint(\"\\nüëá Click the link below to download your full submission üëá\")\nFileLink(r'hackathon_submission.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:21:00.357687Z","iopub.execute_input":"2026-01-18T15:21:00.358080Z","iopub.status.idle":"2026-01-18T15:21:02.457010Z","shell.execute_reply.started":"2026-01-18T15:21:00.358052Z","shell.execute_reply":"2026-01-18T15:21:02.456281Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Zip file created successfully: hackathon_submission.zip\n\nüëá Click the link below to download your full submission üëá\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/hackathon_submission.zip","text/html":"<a href='hackathon_submission.zip' target='_blank'>hackathon_submission.zip</a><br>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import MinMaxScaler\nimport shutil\nimport os\nimport warnings\nfrom IPython.display import FileLink\n\n# ==========================================\n# 0. CONFIGURATION\n# ==========================================\nwarnings.filterwarnings('ignore')\nOUTPUT_DIR = \"hackathon_submission_final\"\nif os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)\nos.makedirs(OUTPUT_DIR)\nFILE_PATH = \"/kaggle/input/aadhar/aadhaar_cleaned.csv\"\n\n# ==========================================\n# 1. ROBUST DATA ENGINE\n# ==========================================\ndef load_engine(path):\n    print(\"üöÄ Booting Grandmaster Analytics Engine...\")\n    try:\n        df = pd.read_csv(path)\n        # Clean Headers (Strip #, spaces)\n        df.columns = df.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # Smart Rename\n        col_map = {}\n        for c in df.columns:\n            if '18' in c: col_map[c] = 'Age_18_greater'\n            elif '0_5' in c: col_map[c] = 'Age_0_5'\n            elif '5_17' in c: col_map[c] = 'Age_5_17'\n        df.rename(columns=col_map, inplace=True)\n        \n        # Force Numeric\n        nums = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for c in nums: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n        \n        df['Total_Reg'] = df[nums].sum(axis=1)\n        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n        df = df[df['Total_Reg'] > 0].copy() # Remove ghosts\n        \n        print(f\"‚úî Data Ready: {len(df):,} records.\")\n        return df\n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")\n        return None\n\n# ==========================================\n# PHASE 1: UNIVARIATE (Temporal & Geo)\n# ==========================================\ndef phase_1_analysis(df):\n    print(\"üìä Phase 1: Univariate Analysis (Trends & Rankings)...\")\n    \n    # 1.1 Temporal: Monthly Trend\n    daily = df.groupby('Date')['Total_Reg'].sum().reset_index().sort_values('Date')\n    fig1 = px.line(daily, x='Date', y='Total_Reg', title=\"<b>1.1 National Enrolment Trend (Time Series)</b>\")\n    fig1.write_html(f\"{OUTPUT_DIR}/Phase1_1_Temporal_Trend.html\")\n    \n    # 1.2 Geographic: State Rankings (Treemap instead of Bar for \"Map\" feel)\n    state_sum = df.groupby('State')['Total_Reg'].sum().reset_index()\n    fig2 = px.treemap(\n        state_sum, path=['State'], values='Total_Reg',\n        title=\"<b>1.2 Geographic Volume Map (State Level)</b>\",\n        color='Total_Reg', color_continuous_scale='Viridis'\n    )\n    fig2.write_html(f\"{OUTPUT_DIR}/Phase1_2_Geo_Treemap.html\")\n    \n    # 1.3 Age Group Distribution\n    ages = df[['Age_0_5', 'Age_5_17', 'Age_18_greater']].sum().reset_index()\n    ages.columns = ['Cohort', 'Count']\n    fig3 = px.pie(ages, names='Cohort', values='Count', title=\"<b>1.3 National Age Demographics</b>\", hole=0.4)\n    fig3.write_html(f\"{OUTPUT_DIR}/Phase1_3_Age_Distribution.html\")\n\n# ==========================================\n# PHASE 2: BIVARIATE (Correlations)\n# ==========================================\ndef phase_2_analysis(df):\n    print(\"üîó Phase 2: Bivariate Analysis (Correlations)...\")\n    \n    # 2.1 Correlation Heatmap\n    corr = df[['Age_0_5', 'Age_5_17', 'Age_18_greater', 'Total_Reg']].corr()\n    fig1 = px.imshow(corr, text_auto=True, title=\"<b>2.1 Demographic Correlation Matrix</b>\", color_continuous_scale='RdBu_r')\n    fig1.write_html(f\"{OUTPUT_DIR}/Phase2_1_Correlation.html\")\n    \n    # 2.2 Lag Analysis (Simulated for \"Update\" delay)\n    # We compare Month N vs Month N-1 growth\n    if df['Date'].nunique() > 10:\n        daily = df.groupby('Date')['Total_Reg'].sum().reset_index()\n        daily['Lag_1'] = daily['Total_Reg'].shift(1)\n        fig2 = px.scatter(\n            daily, x='Lag_1', y='Total_Reg', trendline='ols',\n            title=\"<b>2.2 Temporal Autocorrelation (Lag Plot)</b><br><sub>Consistency Check: High R¬≤ = Stable Operations</sub>\"\n        )\n        fig2.write_html(f\"{OUTPUT_DIR}/Phase2_2_Lag_Analysis.html\")\n\n# ==========================================\n# PHASE 3: TRIVARIATE (Complex 3D)\n# ==========================================\ndef phase_3_analysis(df):\n    print(\"üßä Phase 3: Trivariate Analysis (3D & Matrices)...\")\n    \n    # 3.1 Heatmap Matrix: State x Time x Volume\n    if df['Date'].nunique() > 1:\n        df['Month'] = df['Date'].dt.to_period('M').astype(str)\n        matrix = df.pivot_table(index='State', columns='Month', values='Total_Reg', aggfunc='sum').fillna(0)\n        \n        fig1 = px.imshow(\n            matrix, aspect='auto',\n            title=\"<b>3.1 Spatio-Temporal Matrix (State vs. Time)</b>\",\n            labels=dict(x=\"Timeline\", y=\"State\", color=\"Volume\")\n        )\n        fig1.write_html(f\"{OUTPUT_DIR}/Phase3_1_Heatmap_Matrix.html\")\n        \n    # 3.3 3D Scatter: Pincode x Adult % x Child % (Clustering visual)\n    # Aggregated to District to prevent crash\n    dist = df.groupby(['State', 'District']).agg({'Age_0_5':'sum', 'Age_18_greater':'sum', 'Total_Reg':'sum'}).reset_index()\n    dist['Child_Pct'] = dist['Age_0_5'] / dist['Total_Reg']\n    dist['Adult_Pct'] = dist['Age_18_greater'] / dist['Total_Reg']\n    \n    fig2 = px.scatter_3d(\n        dist, x='Child_Pct', y='Adult_Pct', z='Total_Reg',\n        color='State', size='Total_Reg', size_max=30,\n        title=\"<b>3.3 Multi-Dimensional District Clustering (3D)</b>\",\n        labels={'Child_Pct': 'Child Ratio', 'Adult_Pct': 'Adult Ratio', 'Total_Reg': 'Volume'}\n    )\n    fig2.write_html(f\"{OUTPUT_DIR}/Phase3_3_3D_Clustering.html\")\n\n# ==========================================\n# FEATURE 7 FIXED: DIGITAL DIVIDE (Gini)\n# ==========================================\ndef feature_7_ddi_fixed(df):\n    print(\"‚öñÔ∏è Feature 7: Digital Divide Index (Gini-Based)...\")\n    \n    # DDI = 100 - (Gini Coefficient * 100). Higher is Better.\n    def calculate_ddi(x):\n        if len(x) < 2: return 50 # Default for single-data points\n        # Gini calc\n        sorted_x = np.sort(x)\n        n = len(x)\n        cumx = np.cumsum(sorted_x, dtype=float)\n        gini = (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n        return (1 - gini) * 100\n\n    # Calculate per District based on Pincode distribution\n    ddi_scores = df.groupby(['State', 'District'])['Total_Reg'].apply(lambda x: calculate_ddi(x.values)).reset_index()\n    ddi_scores.rename(columns={'Total_Reg': 'DDI_Score'}, inplace=True)\n    \n    # Bottom 15 (Critical Zones)\n    bottom_15 = ddi_scores.sort_values('DDI_Score').head(15)\n    \n    fig = px.bar(\n        bottom_15, x='DDI_Score', y='District', color='State', orientation='h',\n        title=\"<b>Feature 7: Digital Divide Index (DDI)</b><br><sub>Low Score = High Inequality (Access concentrated in few pincodes)</sub>\",\n        color_discrete_sequence=['#FF5733']\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/Feature7_DDI_Fixed.html\")\n\n# ==========================================\n# FEATURE: TREE DIAGRAMS (Sunburst + Tree)\n# ==========================================\ndef feature_tree_visuals(df):\n    print(\"üå≥ Feature: Generating Tree-Based Visuals...\")\n    \n    # 1. Sunburst (Circular Tree)\n    agg = df.groupby(['State', 'District']).agg({'Total_Reg':'sum', 'Age_0_5':'sum'}).reset_index()\n    agg['Child_Density'] = agg['Age_0_5'] / agg['Total_Reg']\n    \n    fig1 = px.sunburst(\n        agg, path=['State', 'District'], values='Total_Reg', color='Child_Density',\n        title=\"<b>Hierarchical Tree 1: Sunburst (State ‚Üí District)</b>\",\n        color_continuous_scale='RdBu'\n    )\n    fig1.write_html(f\"{OUTPUT_DIR}/Visual_Tree_Sunburst.html\")\n    \n    # 2. Treemap (Rectangular Tree - The \"Map\" substitute)\n    fig2 = px.treemap(\n        agg, path=['State', 'District'], values='Total_Reg', color='Child_Density',\n        title=\"<b>Hierarchical Tree 2: Treemap Analysis</b>\"\n    )\n    fig2.write_html(f\"{OUTPUT_DIR}/Visual_Tree_Treemap.html\")\n\n# ==========================================\n# FEATURE: ANOMALY DETECTION (AI)\n# ==========================================\ndef feature_anomaly_ai(df):\n    print(\"ü§ñ Feature: AI Anomaly Detection...\")\n    features = df[['Age_0_5', 'Age_18_greater']].fillna(0)\n    \n    iso = IsolationForest(contamination=0.01, random_state=42)\n    df['Anomaly'] = iso.fit_predict(features)\n    anomalies = df[df['Anomaly'] == -1]\n    \n    # Plot anomalies\n    fig = px.scatter(\n        df.sample(min(5000, len(df))), x='Age_18_greater', y='Age_0_5', color='Anomaly',\n        title=\"<b>AI Anomaly Detection</b><br><sub>Red points = Unusual Demographic Patterns</sub>\",\n        color_discrete_map={1:'blue', -1:'red'}\n    )\n    fig.write_html(f\"{OUTPUT_DIR}/Feature_AI_Anomaly.html\")\n\n# ==========================================\n# MAIN EXECUTION\n# ==========================================\nif __name__ == \"__main__\":\n    df = load_engine(FILE_PATH)\n    \n    if df is not None:\n        # Run Framework\n        phase_1_analysis(df)\n        phase_2_analysis(df)\n        phase_3_analysis(df)\n        \n        # Run Specialized Features\n        feature_7_ddi_fixed(df)\n        feature_tree_visuals(df)\n        feature_anomaly_ai(df)\n        \n        # ZIP AND LINK\n        print(\"\\nüì¶ Zipping Strategy...\")\n        shutil.make_archive(\"Hackathon_Winner_Submission\", 'zip', OUTPUT_DIR)\n        print(\"‚úÖ DONE! Download below:\")\n        display(FileLink(r'Hackathon_Winner_Submission.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:32:05.470216Z","iopub.execute_input":"2026-01-18T15:32:05.470972Z","iopub.status.idle":"2026-01-18T15:32:27.658429Z","shell.execute_reply.started":"2026-01-18T15:32:05.470944Z","shell.execute_reply":"2026-01-18T15:32:27.657671Z"}},"outputs":[{"name":"stdout","text":"üöÄ Booting Grandmaster Analytics Engine...\n‚úî Data Ready: 1,208,727 records.\nüìä Phase 1: Univariate Analysis (Trends & Rankings)...\nüîó Phase 2: Bivariate Analysis (Correlations)...\nüßä Phase 3: Trivariate Analysis (3D & Matrices)...\n‚öñÔ∏è Feature 7: Digital Divide Index (Gini-Based)...\nüå≥ Feature: Generating Tree-Based Visuals...\nü§ñ Feature: AI Anomaly Detection...\n\nüì¶ Zipping Strategy...\n‚úÖ DONE! Download below:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/Hackathon_Winner_Submission.zip","text/html":"<a href='Hackathon_Winner_Submission.zip' target='_blank'>Hackathon_Winner_Submission.zip</a><br>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ==========================================\n# 1. ROBUST DATA ENGINE (With Before/After Report)\n# ==========================================\ndef load_engine(path):\n    print(\"üöÄ Booting Grandmaster Analytics Engine...\")\n    try:\n        # 1. Load Raw Data\n        df = pd.read_csv(path)\n        raw_count = len(df) # <--- BEFORE COUNT\n        \n        # 2. Clean Headers\n        df.columns = df.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # 3. Rename Columns\n        col_map = {}\n        for c in df.columns:\n            if '18' in c: col_map[c] = 'Age_18_greater'\n            elif '0_5' in c: col_map[c] = 'Age_0_5'\n            elif '5_17' in c: col_map[c] = 'Age_5_17'\n        df.rename(columns=col_map, inplace=True)\n        \n        # 4. Force Numeric & Calculate Totals\n        nums = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for c in nums: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n        \n        df['Total_Reg'] = df[nums].sum(axis=1)\n        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n        \n        # 5. Filter (Remove Ghost Rows)\n        df_clean = df[df['Total_Reg'] > 0].copy()\n        clean_count = len(df_clean) # <--- AFTER COUNT\n        \n        dropped = raw_count - clean_count\n        \n        print(f\"------------------------------------------------\")\n        print(f\"üì• RAW DATA IMPORTED : {raw_count:,} records\")\n        print(f\"üóëÔ∏è DROPPED (Empty/0) : {dropped:,} records\")\n        print(f\"‚úÖ FINAL DATA READY  : {clean_count:,} records\")\n        print(f\"------------------------------------------------\")\n        \n        return df_clean\n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:40:13.453621Z","iopub.execute_input":"2026-01-18T15:40:13.453970Z","iopub.status.idle":"2026-01-18T15:40:13.463469Z","shell.execute_reply.started":"2026-01-18T15:40:13.453936Z","shell.execute_reply":"2026-01-18T15:40:13.462510Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\n\nFILE_PATH = \"/kaggle/input/aadhar/aadhaar_cleaned.csv\"\n\ndef check_data_counts(path):\n    print(\"üöÄ Verifying Data Integrity...\")\n    try:\n        # 1. Load Raw Data (BEFORE)\n        df_raw = pd.read_csv(path)\n        raw_count = len(df_raw)\n        \n        # 2. Simulate the Cleaning Process\n        # Clean Headers\n        df_raw.columns = df_raw.columns.str.replace('#', '', regex=False).str.strip()\n        \n        # Rename Columns\n        col_map = {}\n        for c in df_raw.columns:\n            if '18' in c: col_map[c] = 'Age_18_greater'\n            elif '0_5' in c: col_map[c] = 'Age_0_5'\n            elif '5_17' in c: col_map[c] = 'Age_5_17'\n        df_raw.rename(columns=col_map, inplace=True)\n        \n        # Force Numeric\n        nums = ['Age_0_5', 'Age_5_17', 'Age_18_greater']\n        for c in nums: \n            df_raw[c] = pd.to_numeric(df_raw[c], errors='coerce').fillna(0)\n        \n        # Calculate Total\n        df_raw['Total_Reg'] = df_raw[nums].sum(axis=1)\n        \n        # 3. Filter (AFTER)\n        # We keep only rows with Total_Reg > 0\n        df_clean = df_raw[df_raw['Total_Reg'] > 0]\n        clean_count = len(df_clean)\n        \n        dropped_count = raw_count - clean_count\n        \n        # 4. Print Report\n        print(f\"------------------------------------------------\")\n        print(f\"üì• RAW DATA IMPORTED  : {raw_count:,} records\")\n        print(f\"üóëÔ∏è DROPPED (Empty/0)  : {dropped_count:,} records\")\n        print(f\"‚úÖ FINAL DATA READY   : {clean_count:,} records\")\n        print(f\"------------------------------------------------\")\n        print(f\"DATA RETENTION RATE   : {round((clean_count/raw_count)*100, 4)}%\")\n        print(f\"------------------------------------------------\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error reading file: {e}\")\n\n# Run the check\ncheck_data_counts(FILE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:41:09.173040Z","iopub.execute_input":"2026-01-18T15:41:09.173392Z","iopub.status.idle":"2026-01-18T15:41:12.356147Z","shell.execute_reply.started":"2026-01-18T15:41:09.173363Z","shell.execute_reply":"2026-01-18T15:41:12.355294Z"}},"outputs":[{"name":"stdout","text":"üöÄ Verifying Data Integrity...\n------------------------------------------------\nüì• RAW DATA IMPORTED  : 1,208,847 records\nüóëÔ∏è DROPPED (Empty/0)  : 120 records\n‚úÖ FINAL DATA READY   : 1,208,727 records\n------------------------------------------------\nDATA RETENTION RATE   : 99.9901%\n------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}